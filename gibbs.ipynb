{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import gamma, multivariate_normal\n",
    "from scipy import optimize\n",
    "import numdifftools as nd \n",
    "from scipy.special import expit as logistic\n",
    "\n",
    "class SupervisedPoissonFactorization:\n",
    "    def __init__(self, n, p, d, k, a, a_prime, b_prime, c, c_prime, d_prime, sigma_upsilon, sigma_gamma):\n",
    "        \n",
    "        for param, name in zip([n, p, d, k, a, a_prime, b_prime, c, c_prime, d_prime, sigma_upsilon, sigma_gamma],\n",
    "                               ['n', 'p', 'd', 'k', 'a', 'a_prime', 'b_prime', 'c', 'c_prime', 'd_prime', 'sigma_upsilon', 'sigma_gamma']):\n",
    "            if param <= 0:\n",
    "                raise ValueError(f\"Parameter {name} must be positive.\")\n",
    "\n",
    "        self.n = n #Number of samples\n",
    "        self.p = p #Number of genes\n",
    "        self.d = d #Number of factors\n",
    "        self.k = k #Number of phenotypes\n",
    "        #Hyperparameters:\n",
    "        self.a = a \n",
    "        self.a_prime = a_prime \n",
    "        self.b_prime = b_prime \n",
    "        self.c = c\n",
    "        self.c_prime = c_prime \n",
    "        self.d_prime = d_prime\n",
    "        self.sigma_upsilon = sigma_upsilon\n",
    "        self.sigma_gamma = sigma_gamma\n",
    "        #Parameters\n",
    "        self.xi = np.random.gamma(a_prime, b_prime/a_prime, size=n)\n",
    "        self.eta = np.random.gamma(c_prime, d_prime/c_prime, size=p)\n",
    "        self.theta = np.random.gamma(a, 1/self.xi[:, np.newaxis], size=(n, d))\n",
    "        self.beta = np.random.gamma(c, 1/self.eta[:, np.newaxis], size=(p, d))\n",
    "        self.upsilon = np.random.normal(0, sigma_upsilon, size=(k, d))\n",
    "        self.gamma = np.random.normal(0, sigma_gamma, size=(k, 2)) #the 2 is for the number of aux features, should be updated acc to data\n",
    "\n",
    "    def log_posterior_theta(self, theta_i, x_i, y_i, beta, xi_i, upsilon, gamma, x_aux_i):\n",
    "        log_likelihood = np.sum(x_i * np.log(theta_i @ beta.T) - (theta_i @ beta.T))\n",
    "        log_prior = np.sum((self.a - 1) * np.log(theta_i) - xi_i * theta_i)\n",
    "        log_y = np.sum(y_i * np.log(logistic(theta_i @ upsilon.T + x_aux_i @ gamma.T)) + \n",
    "                       (1 - y_i) * np.log(1 - logistic(theta_i @ upsilon.T + x_aux_i @ gamma.T)))\n",
    "        return log_likelihood + log_prior + log_y\n",
    "    \n",
    "    def log_posterior_upsilon(self, upsilon_k, y_k, theta, gamma_k, x_aux):\n",
    "        log_likelihood = np.sum(y_k * np.log(logistic(theta @ upsilon_k + x_aux @ gamma_k)) + \n",
    "                                (1 - y_k) * np.log(1 - logistic(theta @ upsilon_k + x_aux @ gamma_k)))\n",
    "        log_prior = -0.5 * np.sum(upsilon_k**2) / self.sigma_upsilon**2\n",
    "        return log_likelihood + log_prior\n",
    "\n",
    "    def log_posterior_gamma(self, gamma_k, y_k, theta, upsilon_k, x_aux):\n",
    "        log_likelihood = np.sum(y_k * np.log(logistic(theta @ upsilon_k + x_aux @ gamma_k)) + \n",
    "                                (1 - y_k) * np.log(1 - logistic(theta @ upsilon_k + x_aux @ gamma_k)))\n",
    "        log_prior = -0.5 * np.sum(gamma_k**2) / self.sigma_gamma**2\n",
    "        return log_likelihood + log_prior\n",
    "    \n",
    "    def log_posterior_beta(self, beta_j, x, theta, eta_j):\n",
    "        log_prior = (self.c - 1) * np.log(beta_j) - eta_j * beta_j\n",
    "        log_likelihood = np.sum([x[i, j] * np.log(theta[i].dot(beta_j)) - theta[i].dot(beta_j) for i in range(self.n)])\n",
    "        return log_prior + log_likelihood\n",
    "\n",
    "    def sample_xi(self):\n",
    "        shape = self.a * self.d + self.a_prime \n",
    "        rate = (self.a_prime / self.b_prime) + np.sum(self.theta, axis=1)\n",
    "        self.xi = np.random.gamma(shape, 1/rate)\n",
    "\n",
    "    def sample_eta(self):\n",
    "        shape = self.c * self.d + self.c_prime \n",
    "        rate = (self.c_prime / self.d_prime) + np.sum(self.beta, axis=1)\n",
    "        self.eta = np.random.gamma(shape, 1/rate)\n",
    "\n",
    "    def sample_beta_laplace(self, X, theta, eta):\n",
    "        beta = np.zeros((self.p, self.d))\n",
    "        for j in range(self.p):\n",
    "            neg_log_posterior = lambda b: -self.log_posterior_beta(b, X[:, j], theta, eta[j])\n",
    "            initial_guess = self.beta[j]\n",
    "            result = optimize.minimize(neg_log_posterior, initial_guess, method='BFGS')\n",
    "            if not result.success:\n",
    "                raise RuntimeError(f\"Optimization for beta_{j} failed: {result.message}\")\n",
    "            mode = result.x\n",
    "            hessian = nd.Hessian(neg_log_posterior)(mode)\n",
    "            cov = np.linalg.inv(hessian)\n",
    "            beta[j] = multivariate_normal.rvs(mean=mode, cov=cov)\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_theta_laplace(self, X, y, x_aux, beta, xi, upsilon, gamma):\n",
    "        theta = np.zeros((self.n, self.d))\n",
    "        for i in range(self.n):\n",
    "            neg_log_posterior = lambda t: -self.log_posterior_theta(t, X[i], y[i], beta, xi[i], upsilon, gamma, x_aux[i])\n",
    "            initial_guess = self.theta[i]\n",
    "            result = optimize.minimize(neg_log_posterior, initial_guess, method='BFGS')\n",
    "            if not result.success:\n",
    "                raise RuntimeError(f\"Optimization for theta_{i} failed: {result.message}\")\n",
    "            mode = result.x\n",
    "            hessian = nd.Hessian(neg_log_posterior)(mode)\n",
    "            cov = np.linalg.inv(hessian)\n",
    "            theta[i] = multivariate_normal.rvs(mean=mode, cov=cov)\n",
    "        self.theta = theta\n",
    "\n",
    "    def sample_upsilon_laplace(self, y, theta, gamma, x_aux):\n",
    "        upsilon = np.zeros((self.k, self.d))\n",
    "        for k in range(self.k):\n",
    "            neg_log_posterior = lambda u: -self.log_posterior_upsilon(u, y[:, k], theta, gamma[k], x_aux)\n",
    "            initial_guess = self.upsilon[k]\n",
    "            result = optimize.minimize(neg_log_posterior, initial_guess, method='BFGS')\n",
    "            if not result.success:\n",
    "                raise RuntimeError(f\"Optimization for upsilon_{k} failed: {result.message}\")\n",
    "            mode = result.x\n",
    "            hessian = nd.Hessian(neg_log_posterior)(mode)\n",
    "            cov = np.linalg.inv(hessian)\n",
    "            upsilon[k] = multivariate_normal.rvs(mean=mode, cov=cov)\n",
    "        self.upsilon = upsilon\n",
    "\n",
    "    def sample_gamma_laplace(self, y, theta, upsilon, x_aux):\n",
    "        gamma = np.zeros((self.k, x_aux.shape[1]))\n",
    "        for k in range(self.k):\n",
    "            neg_log_posterior = lambda g: -self.log_posterior_gamma(g, y[:, k], theta, upsilon[k], x_aux)\n",
    "            initial_guess = self.gamma[k]\n",
    "            result = optimize.minimize(neg_log_posterior, initial_guess, method='BFGS')\n",
    "            if not result.success:\n",
    "                raise RuntimeError(f\"Optimization for gamma_{k} failed: {result.message}\")\n",
    "            mode = result.x\n",
    "            hessian = nd.Hessian(neg_log_posterior)(mode)\n",
    "            cov = np.linalg.inv(hessian)\n",
    "            gamma[k] = multivariate_normal.rvs(mean=mode, cov=cov)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def gibbs_step(self, X, y, x_aux):\n",
    "        self.sample_xi()\n",
    "        self.sample_eta()\n",
    "        self.sample_beta_laplace(X, self.theta, self.eta)\n",
    "        self.sample_theta_laplace(X, y, x_aux, self.beta, self.xi, self.upsilon, self.gamma)\n",
    "        self.sample_upsilon_laplace(y, self.theta, self.gamma, x_aux)\n",
    "        self.sample_gamma_laplace(y, self.theta, self.upsilon, x_aux)\n",
    "    \n",
    "    def run_gibbs(self, X, y, x_aux, num_iterations, burn_in):\n",
    "        self._check_input_dimensions(X, y, x_aux)\n",
    "        \n",
    "        if burn_in >= num_iterations:\n",
    "            raise ValueError(\"burn_in must be less than num_iterations\")\n",
    "\n",
    "        samples = []\n",
    "        for i in range(num_iterations):\n",
    "            self.gibbs_step(X, y, x_aux)\n",
    "            if i >= burn_in:\n",
    "                samples.append((self.xi.copy(), self.eta.copy(), self.beta.copy(), \n",
    "                                self.theta.copy(), self.upsilon.copy(), self.gamma.copy()))\n",
    "    \n",
    "        mean_xi = np.mean([s[0] for s in samples], axis=0)\n",
    "        mean_eta = np.mean([s[1] for s in samples], axis=0)\n",
    "        mean_beta = np.mean([s[2] for s in samples], axis=0)\n",
    "        mean_theta = np.mean([s[3] for s in samples], axis=0)\n",
    "        mean_upsilon = np.mean([s[4] for s in samples], axis=0)\n",
    "        mean_gamma = np.mean([s[5] for s in samples], axis=0)\n",
    "        \n",
    "        return mean_xi, mean_eta, mean_beta, mean_theta, mean_upsilon, mean_gamma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
